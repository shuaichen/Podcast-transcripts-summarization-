{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone --- Build Abstractive summarization model.\n",
    "\n",
    "\n",
    "#### Shuaichen Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to Build abstractive summarization model for podcast transcripts.\n",
    "\n",
    "Diffferent from extractive summarization model, abstracitve summarization is to generate short summary that captures the main ideas of a text.\n",
    "The generated summaries potentially contain new phrases and sentences that may not appear in original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for built neuro network\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import rouge\n",
    "\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the half of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I need read the dataset I double cleaned\n",
    "train = joblib.load('second_clean_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32354, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the computation momery and run time neuro network need, I decide to decrease the train dataset to half, choose from the 25% to 75% in length of summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_summary = train['summary'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32354.000000\n",
       "mean        82.199944\n",
       "std         65.661362\n",
       "min         21.000000\n",
       "25%         42.000000\n",
       "50%         63.000000\n",
       "75%         99.000000\n",
       "max        740.000000\n",
       "Name: summary, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ = np.quantile(len_summary,0.25)\n",
    "_end = np.quantile(len_summary,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_half = np.where((len_summary>start_) & (len_summary<_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the half size of data\n",
    "train_half = train.loc[train_half]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_half=pd.read_csv('train_half.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15729, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_half.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of transcripts of the half dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_transcript_half = train_half['transcript'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15729.000000\n",
       "mean      7177.436455\n",
       "std       4023.316547\n",
       "min         27.000000\n",
       "25%       3967.000000\n",
       "50%       6734.000000\n",
       "75%      10030.000000\n",
       "max      22216.000000\n",
       "Name: transcript, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_transcript_half.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central transcripts still have 7177±4023 words, which is compatibel with full size summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_summary_half = train_half['summary'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15729.000000\n",
       "mean        65.613707\n",
       "std         15.519773\n",
       "min         43.000000\n",
       "25%         52.000000\n",
       "50%         63.000000\n",
       "75%         78.000000\n",
       "max         98.000000\n",
       "Name: summary, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_summary_half.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For summries, the central length become 65 ± 15,\n",
    "which is better for build a summarization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put transcript and summary into X and y, split it into train and test dataset.\n",
    "\n",
    "Then, split the train into train and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_half['transcript']\n",
    "y=train_half['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train-validation and test dataset\n",
    "X_, X_test, y_, y_test = train_test_split(X,y,test_size=0.2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_,y_,test_size=0.1,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11324,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11324,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the transcript tokens and encoder size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For I will build RNN sequence to sequence model.\n",
    "The Architecture will be like this:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14iYh3JU7OiMz2--fPFXrEiagw1juHruI\" height=400 width=600></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture seperate the input encoding and output encoding complettely.\n",
    "\n",
    "In this encoder decoder model, the encoder module that processes the input sequence to produce a single vector as a representation for the input.\n",
    "\n",
    "This vector is fed into the decodre module, which generates the output sequence in an auto-regressive fashion.\n",
    "\n",
    "And the first step output of the decoder is fed as the input of the second-step and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the RNN model I will build, a transcript will be the input sequence which will be encoder processed as a single vector later.\n",
    "\n",
    "\n",
    "Encoder model can't read raw text.\n",
    "\n",
    "First I need tokenizer my whole transcript corpus.\n",
    "\n",
    "Second I need use represent my whole transcript corpus with word tokens index.\n",
    "\n",
    "Third I need pad my document sequence, encoder model deal with the same size of document sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/61.jpg\" height=400 width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram[1] above describe how encoder work. At each step, One word is fed into the encoder. At the end, the hidden state h, and the cell state c will be used to initialize the decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of input for recurrent network will be n*q*d:\n",
    "\n",
    "n: the number of obseravation\n",
    "the number of the sequence\n",
    "\n",
    "q: the sequence length\n",
    "\n",
    "d: the number of feature for each element belong to the sequence.\n",
    "\n",
    "In my RNN model, the number of observation will be the train size\n",
    "the sequence length will be the max length of my transcripts\n",
    "the number of feature for each element will be the embeding dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need explore train dataset.\n",
    "\n",
    "Check the rare words for transcripts in train dataset. How much the rare words makes up the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rare words in summary vocabulary:58.79%\n",
      "Total Coverage of rare words for summary:0.22%\n"
     ]
    }
   ],
   "source": [
    "thresh=6 #threshold to define a rare word\n",
    "rare=0 #how many rare words in the corpus\n",
    "unique=0 # how many unique words in the corpus\n",
    "rare_freq=0 # how many times the rare words shows up\n",
    "freq = 0 # how long the whole corpus is\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    unique=unique+1\n",
    "    freq=freq+value\n",
    "    if(value<thresh):\n",
    "        rare=rare+1\n",
    "        rare_freq=rare_freq+value\n",
    "print(f'Percentage of rare words in summary vocabulary:{round(rare/unique*100,2)}%')\n",
    "print(f'Total Coverage of rare words for summary:{round(rare_freq/freq*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define rare word as tokens that show up less than 6 times, in whole corpus.\n",
    "\n",
    "These kind rare words makes up 58% all tokens, but only shows up contribute to 0.23% of corpus.\n",
    "\n",
    "\n",
    "I decide to exclude all the rare words in the tokenizer by set num_words=(unique-rare),\n",
    "\n",
    "When I use function text_to_sequence, only indexes of top num_words frequent words will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I need build new tokenizer\n",
    "x_tokenizer = Tokenizer(num_words=unique-rare)\n",
    "x_tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform word in sentences into index of word token\n",
    "X_train_toseq = x_tokenizer.texts_to_sequences(X_train)\n",
    "#create sequence for validation set\n",
    "X_val_toseq = x_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10030.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consider decrease the parameter to train, set max_x_len as quantile 75%\n",
    "np.quantile(len_transcript_half,0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x_len = 10030 #75% of the transcripts are lower than 10030 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pading zero up tp maximum length of sequence\n",
    "X_train_pad = pad_sequences(X_train_toseq, maxlen = max_x_len, padding = 'post')\n",
    "#pading zero up tp maximum length of sequence\n",
    "X_val_pad = pad_sequences(X_val_toseq, maxlen = max_x_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_train_pad.pkl']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train_pad,'x_train_pad.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_val_pad.pkl']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_val_pad,'x_val_pad.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can do the same thing to summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need add start and end to the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Diagram describe the decoder architecture, which reads the entire target sequence word by word and predicts \n",
    "the same sequence offset by one timestep.\n",
    "\n",
    "The encoder is trained to predict the next word in the sequence given the previous word.\n",
    "\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/05/71.jpg\" height=400 width=600></img>\n",
    "\n",
    "\n",
    "Start and end are the special tokens which are added to the target sequence before feeding into decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add start and end tokens to the y_train\n",
    "y_train = y_train.apply(lambda x: 'sostok '+ x + ' eostok')\n",
    "y_val = y_val.apply(lambda x: 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2065    sostok now, i love chinese food. i mean, who d...\n",
       "5936    sostok time blocking has practically become a ...\n",
       "1406    sostok tonight i am joined by duracell battery...\n",
       "4431    sostok please bear with us for the choppy edit...\n",
       "4764    sostok before cutting ties release their debut...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852      sostok in this episode,​ i am joined by rich r...\n",
       "4701     sostok and we are back! happy new year to you ...\n",
       "11761    sostok fresh off the packers' win over the vik...\n",
       "11475    sostok well, here it is! 2012 and all that hap...\n",
       "15712    sostok lean in as jasmine, business owner, mot...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rare words in summary vocabulary:81.43%\n",
      "Total Coverage of rare words for summary:7.73%\n"
     ]
    }
   ],
   "source": [
    "thresh=6 #threshold to define a rare word\n",
    "rare=0 #how many rare words in the corpus\n",
    "unique=0 # how many unique words in the corpus\n",
    "rare_freq=0 # how many times the rare words shows up\n",
    "freq = 0 # how long the whole corpus is\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    unique=unique+1\n",
    "    freq=freq+value\n",
    "    if(value<thresh):\n",
    "        rare=rare+1\n",
    "        rare_freq=rare_freq+value\n",
    "print(f'Percentage of rare words in summary vocabulary:{round(rare/unique*100,2)}%')\n",
    "print(f'Total Coverage of rare words for summary:{round(rare_freq/freq*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For summary, the rare words take up to 81% of all tokens, but only contribute to 7% of corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the situation, I build a new tokenizer\n",
    "y_tokenizer = Tokenizer(num_words=unique-rare)\n",
    "y_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_index['sostok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_index['eostok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform word in sentences into index of word token\n",
    "y_train_toseq = y_tokenizer.texts_to_sequences(y_train)\n",
    "#create sequence for validation set\n",
    "y_val_toseq = y_tokenizer.texts_to_sequences(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y_len = max(len_summary_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pading zero up tp maximum length of sequence\n",
    "y_train_pad = pad_sequences(y_train_toseq, maxlen = max_y_len, padding = 'post')\n",
    "#pading zero up tp maximum length of sequence\n",
    "y_val_pad = pad_sequences(y_val_toseq, maxlen = max_y_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First One LSTM layer for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts vocabulary is 64751, Summaries vocabulary is 8395.\n"
     ]
    }
   ],
   "source": [
    "x_voc = x_tokenizer.num_words+1\n",
    "y_voc = y_tokenizer.num_words+1\n",
    "print(f'Transcripts vocabulary is {x_voc}, Summaries vocabulary is {y_voc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model,  I set dropout = 0.4, epoches = 10, and batch_size = 128 in this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dropout = 0.2\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "embedding_dim = 100\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoder: x input initiate\n",
    "x_in = Input(shape=(max_x_len,))\n",
    "#embedding\n",
    "layer_x_emb = Embedding(x_voc,embedding_dim,trainable=True)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "##lstm\n",
    "layer_x_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout)\n",
    "x_out, state_h, state_c = layer_x_lstm(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "#y input initiate\n",
    "y_in = Input(shape=(None,))\n",
    "#embedding layer\n",
    "layer_y_emb = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "#lstm\n",
    "layer_y_lstm = LSTM(latent_dim, return_sequences =True, return_state= True, dropout = dropout)\n",
    "y_out, y_h2,  y_c2 = layer_y_lstm(y_emb,initial_state=[state_h,state_c])\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "y_out = decoder_dense(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10030)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10030, 100)   6475100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    839500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 10030, 100), 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 8395)   847895      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,323,295\n",
      "Trainable params: 8,323,295\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = Model([x_in, y_in], y_out)\n",
    "# Display its summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy') #classifier\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2) #epoch early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "89/89 [==============================] - 70s 790ms/step - loss: 5.2964 - val_loss: 4.2768\n",
      "Epoch 2/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 4.2443 - val_loss: 4.0718\n",
      "Epoch 3/50\n",
      "89/89 [==============================] - 69s 781ms/step - loss: 4.1228 - val_loss: 3.9855\n",
      "Epoch 4/50\n",
      "89/89 [==============================] - 70s 784ms/step - loss: 4.0378 - val_loss: 3.9120\n",
      "Epoch 5/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.9587 - val_loss: 3.8350\n",
      "Epoch 6/50\n",
      "89/89 [==============================] - 69s 778ms/step - loss: 3.8745 - val_loss: 3.7615\n",
      "Epoch 7/50\n",
      "89/89 [==============================] - 70s 783ms/step - loss: 3.7983 - val_loss: 3.6958\n",
      "Epoch 8/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 3.7351 - val_loss: 3.6477\n",
      "Epoch 9/50\n",
      "89/89 [==============================] - 69s 775ms/step - loss: 3.6835 - val_loss: 3.6083\n",
      "Epoch 10/50\n",
      "89/89 [==============================] - 70s 789ms/step - loss: 3.6393 - val_loss: 3.5752\n",
      "Epoch 11/50\n",
      "89/89 [==============================] - 70s 784ms/step - loss: 3.5995 - val_loss: 3.5456\n",
      "Epoch 12/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.5644 - val_loss: 3.5202\n",
      "Epoch 13/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.5324 - val_loss: 3.4991\n",
      "Epoch 14/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.5033 - val_loss: 3.4794\n",
      "Epoch 15/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.4768 - val_loss: 3.4613\n",
      "Epoch 16/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.4523 - val_loss: 3.4458\n",
      "Epoch 17/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.4296 - val_loss: 3.4339\n",
      "Epoch 18/50\n",
      "89/89 [==============================] - 70s 785ms/step - loss: 3.4085 - val_loss: 3.4212\n",
      "Epoch 19/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.3885 - val_loss: 3.4080\n",
      "Epoch 20/50\n",
      "89/89 [==============================] - 69s 778ms/step - loss: 3.3696 - val_loss: 3.3981\n",
      "Epoch 21/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.3516 - val_loss: 3.3882\n",
      "Epoch 22/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.3346 - val_loss: 3.3801\n",
      "Epoch 23/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.3183 - val_loss: 3.3705\n",
      "Epoch 24/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 3.3025 - val_loss: 3.3646\n",
      "Epoch 25/50\n",
      "89/89 [==============================] - 70s 782ms/step - loss: 3.2876 - val_loss: 3.3574\n",
      "Epoch 26/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 3.2730 - val_loss: 3.3507\n",
      "Epoch 27/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.2593 - val_loss: 3.3456\n",
      "Epoch 28/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.2456 - val_loss: 3.3401\n",
      "Epoch 29/50\n",
      "89/89 [==============================] - 69s 778ms/step - loss: 3.2328 - val_loss: 3.3355\n",
      "Epoch 30/50\n",
      "89/89 [==============================] - 69s 777ms/step - loss: 3.2205 - val_loss: 3.3317\n",
      "Epoch 31/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 3.2085 - val_loss: 3.3272\n",
      "Epoch 32/50\n",
      "89/89 [==============================] - 70s 784ms/step - loss: 3.1971 - val_loss: 3.3260\n",
      "Epoch 33/50\n",
      "89/89 [==============================] - 70s 781ms/step - loss: 3.1857 - val_loss: 3.3203\n",
      "Epoch 34/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.1754 - val_loss: 3.3168\n",
      "Epoch 35/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.1642 - val_loss: 3.3149\n",
      "Epoch 36/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.1543 - val_loss: 3.3121\n",
      "Epoch 37/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.1444 - val_loss: 3.3097\n",
      "Epoch 38/50\n",
      "89/89 [==============================] - 70s 783ms/step - loss: 3.1346 - val_loss: 3.3085\n",
      "Epoch 39/50\n",
      "89/89 [==============================] - 70s 784ms/step - loss: 3.1255 - val_loss: 3.3093\n",
      "Epoch 40/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.1167 - val_loss: 3.3051\n",
      "Epoch 41/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.1076 - val_loss: 3.3039\n",
      "Epoch 42/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.0990 - val_loss: 3.3016\n",
      "Epoch 43/50\n",
      "89/89 [==============================] - 70s 782ms/step - loss: 3.0908 - val_loss: 3.3014\n",
      "Epoch 44/50\n",
      "89/89 [==============================] - 69s 779ms/step - loss: 3.0823 - val_loss: 3.3016\n",
      "Epoch 45/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.0746 - val_loss: 3.3004\n",
      "Epoch 46/50\n",
      "89/89 [==============================] - 70s 786ms/step - loss: 3.0669 - val_loss: 3.2995\n",
      "Epoch 47/50\n",
      "89/89 [==============================] - 70s 782ms/step - loss: 3.0587 - val_loss: 3.2996\n",
      "Epoch 48/50\n",
      "89/89 [==============================] - 69s 780ms/step - loss: 3.0513 - val_loss: 3.3001\n",
      "Epoch 00048: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([X_train_pad,y_train_pad[:,:-1]],\\\n",
    "                  y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,\\\n",
    "                  epochs=epochs,\\\n",
    "                  callbacks=[es],\\\n",
    "                  batch_size=batch_size, \\\n",
    "                  validation_data=([X_val_pad,y_val_pad[:,:-1]], \\\n",
    "                                   y_val_pad.reshape(y_val_pad.shape[0],y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val_loss')\n",
    "plt.title('The Val_loss by every epoch')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the First Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_index_word = x_tokenizer.index_word\n",
    "y_index_word = y_tokenizer.index_word\n",
    "y_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10030)]           0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 10030, 100)        6475100   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 10030, 100), (Non 80400     \n",
      "=================================================================\n",
      "Total params: 6,555,500\n",
      "Trainable params: 6,555,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder inference\n",
    "encoder_model = Model(inputs = x_in, outputs = [x_out, state_h, state_c])\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: en1/assets\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('en1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    839500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       embedding_1[1][0]                \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10030, 100)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 8395)   847895      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,767,795\n",
      "Trainable params: 1,767,795\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder inference\n",
    "\n",
    "encoder_out = Input(shape=(max_x_len,latent_dim))                 \n",
    "decoder_state_h = Input(shape=(latent_dim,))\n",
    "decoder_state_c = Input(shape=(latent_dim,))\n",
    "                               \n",
    "# decoder embedding                \n",
    "dec_emb = layer_y_emb(y_in)\n",
    "                \n",
    "#lstm to predict the next word                \n",
    "dec_out2, dec_state_h2, dec_state_c2 = layer_y_lstm(dec_emb, initial_state = [decoder_state_h, decoder_state_c])\n",
    "                \n",
    "#use softmax to generate probability over vocabular\n",
    "probas = decoder_dense(dec_out2)\n",
    "                \n",
    "#compile\n",
    "decoder_model = Model(inputs=[y_in, encoder_out, decoder_state_h, decoder_state_c], outputs=[probas, dec_state_h2, dec_state_c2])\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: de1/assets\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save('de1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict for one test pad\n",
    "def decode_padding(x):\n",
    "    x=x.reshape(1,max_x_len)\n",
    "    #encoder X\n",
    "    e_out, e_state_h, e_state_c = encoder_model.predict(x)\n",
    "    #The word feed in \n",
    "    target_seq = np.array([y_word_index['sostok']])\n",
    "    predicted_text=\"\"\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        #predict the dictionary probability\n",
    "        tokens, new_state_h, new_state_c = decoder_model.predict([target_seq, e_out, e_state_h, e_state_c])\n",
    "        #get predict word\n",
    "        voc_idx = np.argmax(tokens[0,-1,:])\n",
    "        pred_word = y_index_word[voc_idx]\n",
    "\n",
    "        #check the stop\n",
    "        if (pred_word != 'eostok') and (len(predicted_text.split()) < max_y_len):\n",
    "            predicted_text = predicted_text + \" \" + pred_word\n",
    "        else:\n",
    "            stop=True\n",
    "        #next\n",
    "        target_seq = np.array([voc_idx])\n",
    "        e_state_h, e_state_c = new_state_h, new_state_c\n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in this episode we talk about the importance of the and the most important things we have learned from the world of the world and how to make it and how to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we have to do we'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_padding(X_val_pad[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary generated, I know basically the encoder and decoder layer didn't learn much through the training phase, All the predicted summary are basic the same and basically not related to original summary.\n",
    "\n",
    "Based on the situation, I go through some paper, I found out LSTMs work well if the problem has one output for every input, like time series foresting or text translation.\n",
    "\n",
    "LSTMs can be chanllenging to use, when the input sequences are very long and the outputs are much shorter, which exactly is in our case.\n",
    "\n",
    "I decide manually truncate it to half the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_voc = \n",
    "# y_voc =\n",
    "max_x_len = 5000\n",
    "max_y_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some hyperparameter tuning And Normalization layers.\n",
    "\n",
    "Try to run 50 epochs, Decrease batch_size to 64, and change the dropout as 0.4 and also I add recurrent_regularizer=regularizers.l2(0.01).\n",
    "\n",
    "And I add Four batch normalization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.4\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "embedding_dim = 100 #decrease embedding dimension will decrease running time\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#And batch normalization \n",
    "bn1 = BatchNormalization()\n",
    "bn2 = BatchNormalization()\n",
    "\n",
    "#encoder\n",
    "x_in = Input(shape=(max_x_len,))\n",
    "\n",
    "#embedding\n",
    "layer_x_emb = Embedding(x_voc,embedding_dim,trainable=True)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "x_emb = bn1(x_emb)\n",
    "\n",
    "##lstm\n",
    "layer_x_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout, recurrent_regularizer=regularizers.l2(0.01))\n",
    "x_out, state_h, state_c = layer_x_lstm(x_emb)\n",
    "x_out = bn2(x_out)\n",
    "\n",
    "#decoder\n",
    "#And batch normalization \n",
    "bn3 = BatchNormalization()\n",
    "bn4 = BatchNormalization()\n",
    "\n",
    "#\n",
    "y_in = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "layer_y_emb = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "\n",
    "y_emb = layer_y_emb(y_in)\n",
    "y_emb = bn3(y_emb)\n",
    "\n",
    "#lstm\n",
    "layer_y_lstm = LSTM(latent_dim, return_sequences =True, return_state= True, dropout = dropout, recurrent_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "y_out, y_h,  y_c = layer_y_lstm(y_emb,initial_state=[state_h,state_c])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "\n",
    "y_out = bn4(y_out)\n",
    "y_out = decoder_dense(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 5000, 100)    6475100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    839500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 5000, 100)    400         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 100)    400         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 5000, 100),  80400       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       batch_normalization_2[0][0]      \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100)    400         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 8395)   847895      batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 8,324,495\n",
      "Trainable params: 8,323,895\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = Model([x_in, y_in], y_out)\n",
    "# Display its summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "checkpoint_filepath='./checkpoint1/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5000) for input Tensor(\"input_1:0\", shape=(None, 5000), dtype=float32), but it was called on an input with incompatible shape (None, 10030).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 5000) for input Tensor(\"input_1:0\", shape=(None, 5000), dtype=float32), but it was called on an input with incompatible shape (None, 10030).\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.8680WARNING:tensorflow:Model was constructed with shape (None, 5000) for input Tensor(\"input_1:0\", shape=(None, 5000), dtype=float32), but it was called on an input with incompatible shape (None, 10030).\n",
      "89/89 [==============================] - 72s 811ms/step - loss: 5.8680 - val_loss: 8.2115\n",
      "Epoch 2/50\n",
      "89/89 [==============================] - 71s 801ms/step - loss: 3.9360 - val_loss: 7.6508\n",
      "Epoch 3/50\n",
      "89/89 [==============================] - 72s 804ms/step - loss: 3.6727 - val_loss: 5.6649\n",
      "Epoch 4/50\n",
      "89/89 [==============================] - 72s 807ms/step - loss: 3.5534 - val_loss: 4.0789\n",
      "Epoch 5/50\n",
      "89/89 [==============================] - 72s 804ms/step - loss: 3.4738 - val_loss: 3.6650\n",
      "Epoch 6/50\n",
      "89/89 [==============================] - 71s 801ms/step - loss: 3.4142 - val_loss: 3.4579\n",
      "Epoch 7/50\n",
      "89/89 [==============================] - 72s 804ms/step - loss: 3.3666 - val_loss: 3.3748\n",
      "Epoch 8/50\n",
      "89/89 [==============================] - 71s 803ms/step - loss: 3.3248 - val_loss: 3.3368\n",
      "Epoch 9/50\n",
      "89/89 [==============================] - 71s 801ms/step - loss: 3.2916 - val_loss: 3.3308\n",
      "Epoch 10/50\n",
      "89/89 [==============================] - 72s 807ms/step - loss: 3.2606 - val_loss: 3.2979\n",
      "Epoch 11/50\n",
      "89/89 [==============================] - 71s 798ms/step - loss: 3.2342 - val_loss: 3.3096\n",
      "Epoch 12/50\n",
      "89/89 [==============================] - 71s 798ms/step - loss: 3.2103 - val_loss: 3.3065\n",
      "Epoch 13/50\n",
      "89/89 [==============================] - 71s 796ms/step - loss: 3.1874 - val_loss: 3.2982\n",
      "Epoch 14/50\n",
      "89/89 [==============================] - 71s 802ms/step - loss: 3.1692 - val_loss: 3.2904\n",
      "Epoch 15/50\n",
      "89/89 [==============================] - 71s 803ms/step - loss: 3.1508 - val_loss: 3.2854\n",
      "Epoch 16/50\n",
      "89/89 [==============================] - 71s 801ms/step - loss: 3.1345 - val_loss: 3.3079\n",
      "Epoch 17/50\n",
      "89/89 [==============================] - 71s 799ms/step - loss: 3.1192 - val_loss: 3.2878\n",
      "Epoch 18/50\n",
      "89/89 [==============================] - 71s 797ms/step - loss: 3.1049 - val_loss: 3.2875\n",
      "Epoch 19/50\n",
      "89/89 [==============================] - 71s 799ms/step - loss: 3.0921 - val_loss: 3.3058\n",
      "Epoch 20/50\n",
      "89/89 [==============================] - 71s 800ms/step - loss: 3.0776 - val_loss: 3.2895\n",
      "Epoch 21/50\n",
      "89/89 [==============================] - 71s 801ms/step - loss: 3.0672 - val_loss: 3.2853\n",
      "Epoch 22/50\n",
      "89/89 [==============================] - 71s 799ms/step - loss: 3.0557 - val_loss: 3.2847\n",
      "Epoch 23/50\n",
      "42/89 [=============>................] - ETA: 35s - loss: 3.0343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-7dd7801028dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history=model.fit([X_train_pad,y_train_pad[:,:-1]],\\\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0my_train_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit([X_train_pad,y_train_pad[:,:-1]],\\\n",
    "                  y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,\\\n",
    "                  epochs=epochs,\\\n",
    "                  callbacks=[model_checkpoint_callback],\\\n",
    "                  batch_size=batch_size , \\\n",
    "                  validation_data=([X_val_pad,y_val_pad[:,:-1]], \\\n",
    "                                   y_val_pad.reshape(y_val_pad.shape[0],y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stopped when there are no more improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting oberservation during hyperparametr tuning.\n",
    "\n",
    "When I set batch size small, the ETA also get much higher number.\n",
    "\n",
    "The embedding layer is the make ups marjority of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Inference for Model with Batch-normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 5000, 100)         6475100   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 5000, 100)         400       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 5000, 100), (None 80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5000, 100)         400       \n",
      "=================================================================\n",
      "Total params: 6,556,300\n",
      "Trainable params: 6,555,900\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder inference\n",
    "encoder_model = Model(inputs = x_in, outputs = [x_out, state_h, state_c])\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: en2/assets\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('en2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    839500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 100)    400         embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  80400       batch_normalization_2[2][0]      \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100)    400         lstm_1[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 5000, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 8395)   847895      batch_normalization_3[2][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,768,595\n",
      "Trainable params: 1,768,195\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder inference\n",
    "#initiate the input for decoder model\n",
    "encoder_out = Input(shape=(max_x_len,latent_dim))                    \n",
    "decoder_state_h = Input(shape=(latent_dim,))\n",
    "decoder_state_c = Input(shape=(latent_dim,))\n",
    "                                \n",
    "# decoder embedding                \n",
    "dec_emb = layer_y_emb(y_in)\n",
    "dec_emb = bn3(dec_emb)                \n",
    "#lstm to predict the next word                \n",
    "dec_out2, dec_state_h2, dec_state_c2 = layer_y_lstm(dec_emb, initial_state = [decoder_state_h, decoder_state_c])\n",
    "dec_out2 = bn4(dec_out2)                \n",
    "#use softmax to generate probability over vocabular\n",
    "probas = decoder_dense(dec_out2)                \n",
    "#compile\n",
    "decoder_model = Model(inputs=[y_in, encoder_out, decoder_state_h, decoder_state_c], outputs=[probas, dec_state_h2, dec_state_c2])\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: de2/assets\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save('de2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the guys are back in the first episode of the season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming season of the week we talk about the upcoming'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_padding(X_train_pad[890][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is a good friend and a good friend and a friend of mine and we talk about the importance of being a good friend and how to get a little bit of the best and how to make it a good time to get a little bit of the best and bad things that is not only the best thing you can find out what you think'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_padding(X_train_pad[1200][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' on this episode of the guys discuss the recent trip to the nfl playoffs and the guys discuss the recent trip to the nfl playoffs and the guys discuss the upcoming season of the season and the guys discuss the upcoming season of the season and the guys discuss the upcoming season of the season'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_padding(X_train_pad[2499][:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result proved my assumption, that the encoder and decoder can't handle long sequence, After I cut sequence into half, the quality of summaries spike up.\n",
    "\n",
    "Now the question become How to shorten the sequence. \n",
    "\n",
    "Based the extractive models I built, Baseline model and Text rank, TFIDF, Count Vectorizer model,\n",
    "I found that simply Baseline model and Text rank out performed others in F1 SCores.\n",
    "\n",
    "I wanna try those two:\n",
    "\n",
    "1, the first 5000 words\n",
    "\n",
    "2, text rank, word_count = 5000\n",
    "\n",
    "I decide to try the baseline model first, which is to pick first K words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncate Text Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need redo the tokenizer and everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1000\n",
    "First_500 = train_half['transcript'].apply(lambda x : ' '.join(x.split()[:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=First_500\n",
    "y=train_half['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train-validation and test dataset\n",
    "X_, X_test, y_, y_test = train_test_split(X,y,test_size=0.2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_,y_,test_size=0.1,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform word in sentences into index of word token\n",
    "X_train_toseq = x_tokenizer.texts_to_sequences(X_train)\n",
    "#create sequence for validation set\n",
    "X_val_toseq = x_tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x_len=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pading zero up tp maximum length of sequence\n",
    "X_train_pad = pad_sequences(X_train_toseq, maxlen = max_x_len, padding = 'post')\n",
    "#pading zero up tp maximum length of sequence\n",
    "X_val_pad = pad_sequences(X_val_toseq, maxlen = max_x_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add start and end tokens to the y_train\n",
    "y_train = y_train.apply(lambda x: 'sostok '+ x + ' eostok')\n",
    "y_val = y_val.apply(lambda x: 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2065    sostok now, i love chinese food. i mean, who d...\n",
       "5936    sostok time blocking has practically become a ...\n",
       "1406    sostok tonight i am joined by duracell battery...\n",
       "4431    sostok please bear with us for the choppy edit...\n",
       "4764    sostok before cutting ties release their debut...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y_len = max(len_summary_half) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pading zero up tp maximum length of sequence\n",
    "y_train_pad = pad_sequences(y_train_toseq, maxlen = max_y_len, padding = 'post')\n",
    "#pading zero up tp maximum length of sequence\n",
    "y_val_pad = pad_sequences(y_val_toseq, maxlen = max_y_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts vocabulary is 80308, Summaries vocabulary is 45195.\n"
     ]
    }
   ],
   "source": [
    "x_voc = len(x_tokenizer.index_word)+1\n",
    "y_voc = len(y_tokenizer.index_word)+1\n",
    "print(f'Transcripts vocabulary is {x_voc}, Summaries vocabulary is {y_voc}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcripts vocabulary decrease so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.4\n",
    "r_dropout = 0\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "embedding_dim = 100 #decrease embedding dimension will decrease running time\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#And batch normalization \n",
    "bn1 = BatchNormalization()\n",
    "bn2 = BatchNormalization()\n",
    "bn3 = BatchNormalization()\n",
    "bn4 = BatchNormalization()\n",
    "\n",
    "\n",
    "#encoder\n",
    "x_in = Input(shape=(max_x_len,))\n",
    "\n",
    "#embedding\n",
    "layer_x_emb = Embedding(x_voc,embedding_dim,trainable=True)\n",
    "x_emb = layer_x_emb(x_in)\n",
    "x_emb = bn1(x_emb)\n",
    "\n",
    "##lstm\n",
    "layer_x_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout, recurrent_regularizer=regularizers.l2(0.01))\n",
    "x_out3, x_state_h3, x_state_c3 = layer_x_lstm(x_emb)\n",
    "\n",
    "##lstm 2\n",
    "layer_x_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout, recurrent_regularizer=regularizers.l2(0.01))\n",
    "x_out2, x_state_h2, x_state_c2 = layer_x_lstm2(x_out3)\n",
    "\n",
    "##lstm 3\n",
    "layer_x_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=dropout, recurrent_regularizer=regularizers.l2(0.01))\n",
    "x_out, x_state_h, x_state_c = layer_x_lstm3(x_out2)\n",
    "x_out = bn3(x_out)\n",
    "\n",
    "#decoder\n",
    "y_in = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "layer_y_emb = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "y_emb = layer_y_emb(y_in)\n",
    "y_emb = bn3(y_emb)\n",
    "\n",
    "#lstm\n",
    "layer_y_lstm = LSTM(latent_dim, return_sequences =True, return_state= True, dropout = dropout, recurrent_dropout=r_dropout,recurrent_regularizer=regularizers.l2(0.01))\n",
    "y_out, y_h,  y_c = layer_y_lstm(y_emb,initial_state=[x_state_h,x_state_c])\n",
    "y_out = bn4(y_out)\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "y_out = decoder_dense(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 5000, 100)    8030800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 5000, 100)    400         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 5000, 100),  80400       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    4519500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 5000, 100),  80400       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor multiple             400         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 5000, 100),  80400       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 100),  80400       batch_normalization_2[1][0]      \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100)    400         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 45195)  4564695     batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 17,437,795\n",
      "Trainable params: 17,437,195\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = Model([x_in, y_in], y_out)\n",
    "# Display its summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2) #epoch early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "89/89 [==============================] - 99s 1s/step - loss: 7.4253 - val_loss: 9.3810\n",
      "Epoch 2/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 4.1403 - val_loss: 7.8634\n",
      "Epoch 3/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.8027 - val_loss: 6.0057\n",
      "Epoch 4/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.6452 - val_loss: 4.2608\n",
      "Epoch 5/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.5458 - val_loss: 3.7304\n",
      "Epoch 6/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.4754 - val_loss: 3.5024\n",
      "Epoch 7/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.4208 - val_loss: 3.4098\n",
      "Epoch 8/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.3745 - val_loss: 3.3675\n",
      "Epoch 9/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.3352 - val_loss: 3.3402\n",
      "Epoch 10/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.3034 - val_loss: 3.3180\n",
      "Epoch 11/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.2739 - val_loss: 3.3087\n",
      "Epoch 12/50\n",
      "89/89 [==============================] - 97s 1s/step - loss: 3.2471 - val_loss: 3.3331\n",
      "Epoch 13/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.2250 - val_loss: 3.3075\n",
      "Epoch 14/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.2040 - val_loss: 3.2904\n",
      "Epoch 15/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.1839 - val_loss: 3.2940\n",
      "Epoch 16/50\n",
      "89/89 [==============================] - 98s 1s/step - loss: 3.1677 - val_loss: 3.2943\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([X_train_pad,y_train_pad[:,:-1]],\\\n",
    "                  y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,\\\n",
    "                  epochs=epochs,\\\n",
    "                  callbacks=[es],\\\n",
    "                  batch_size=batch_size , \\\n",
    "                  validation_data=([X_val_pad,y_val_pad[:,:-1]], \\\n",
    "                                   y_val_pad.reshape(y_val_pad.shape[0],y_val_pad.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val_loss')\n",
    "plt.title('The Val_loss by every epoch')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 5000, 100)         5357200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5000, 100)         400       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                [(None, 5000, 100), (None 80400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                [(None, 5000, 100), (None 80400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                [(None, 5000, 100), (None 80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  400       \n",
      "=================================================================\n",
      "Total params: 5,599,200\n",
      "Trainable params: 5,598,800\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#encoder inference\n",
    "encoder_model = Model(inputs = x_in, outputs = [x_out, x_state_h, x_state_c])\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    4519500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor multiple             400         embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 100),  80400       batch_normalization_2[2][0]      \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100)    400         lstm_3[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 5000, 100)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 45195)  4564695     batch_normalization_3[1][0]      \n",
      "==================================================================================================\n",
      "Total params: 9,165,395\n",
      "Trainable params: 9,164,995\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder inference\n",
    "#initiate the input for decoder model\n",
    "encoder_out = Input(shape=(max_x_len,latent_dim))                    \n",
    "decoder_state_h = Input(shape=(latent_dim,))\n",
    "decoder_state_c = Input(shape=(latent_dim,))\n",
    "                                \n",
    "# decoder embedding                \n",
    "dec_emb = layer_y_emb(y_in)\n",
    "dec_emb = bn3(dec_emb)                \n",
    "#lstm to predict the next word                \n",
    "dec_out2, dec_state_h2, dec_state_c2 = layer_y_lstm(dec_emb, initial_state = [decoder_state_h, decoder_state_c])\n",
    "dec_out2 = bn4(dec_out2)                \n",
    "#use softmax to generate probability over vocabular\n",
    "probas = decoder_dense(dec_out2)                \n",
    "#compile\n",
    "decoder_model = Model(inputs=[y_in, encoder_out, decoder_state_h, decoder_state_c], outputs=[probas, dec_state_h2, dec_state_c2])\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  indices[0,444] = 75045 is not in [0, 53572)\n\t [[node functional_5/embedding_2/embedding_lookup (defined at <ipython-input-64-f370cd0f122a>:5) ]]\n  (1) Invalid argument:  indices[0,444] = 75045 is not in [0, 53572)\n\t [[node functional_5/embedding_2/embedding_lookup (defined at <ipython-input-64-f370cd0f122a>:5) ]]\n\t [[functional_5/embedding_2/embedding_lookup/_6]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_206668]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_5/embedding_2/embedding_lookup:\n functional_5/embedding_2/embedding_lookup/205306 (defined at /usr/lib/python3.8/contextlib.py:113)\n\nInput Source operations connected to node functional_5/embedding_2/embedding_lookup:\n functional_5/embedding_2/embedding_lookup/205306 (defined at /usr/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function -> predict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-5d125558c590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m789\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-f370cd0f122a>\u001b[0m in \u001b[0;36mdecode_padding\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_x_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#encoder X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0me_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_state_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#The word feed in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_word_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sostok'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[0,444] = 75045 is not in [0, 53572)\n\t [[node functional_5/embedding_2/embedding_lookup (defined at <ipython-input-64-f370cd0f122a>:5) ]]\n  (1) Invalid argument:  indices[0,444] = 75045 is not in [0, 53572)\n\t [[node functional_5/embedding_2/embedding_lookup (defined at <ipython-input-64-f370cd0f122a>:5) ]]\n\t [[functional_5/embedding_2/embedding_lookup/_6]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_206668]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_5/embedding_2/embedding_lookup:\n functional_5/embedding_2/embedding_lookup/205306 (defined at /usr/lib/python3.8/contextlib.py:113)\n\nInput Source operations connected to node functional_5/embedding_2/embedding_lookup:\n functional_5/embedding_2/embedding_lookup/205306 (defined at /usr/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function -> predict_function\n"
     ]
    }
   ],
   "source": [
    "decode_padding(X_val_pad[789])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "IN this notebook, I build three Neural network models, two without attention layers, one with attention layers.\n",
    "I did some hyperparameter tuning for all the neural network models.\n",
    "\n",
    "dropout would help with overfitting.\n",
    "Batch normalization could acts as regularization and also could help with overfitting.\n",
    "\n",
    "I also add layer regularizaiton for LSTM layer ---l2,regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference\n",
    "\n",
    "[1]Seq2Seq: Abstractive Summarization Using LSTM and Attention Mechanism. Madhav Mishra. Aug 14,2020.\n",
    "https://medium.com/analytics-vidhya/seq2seq-abstractive-summarization-using-lstm-and-attention-mechanism-code-da2e9c439711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
