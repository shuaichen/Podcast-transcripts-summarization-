{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887cced5-0711-430d-985b-b09aec1dd2f7",
   "metadata": {},
   "source": [
    "## Capstone --- Baseline models and Extractive summary models.\n",
    "\n",
    "\n",
    "#### Shuaichen Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad58ac-dd0f-4865-a4df-02319d23a3be",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to build baseline models and extractive summary models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf0111-293f-447e-b6f8-25ab16650e69",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c049a8-dc8a-4d00-b084-1034413b4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f992f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf4eaf-83ef-4ce3-ae88-ca81cfd6c796",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08142903-f72b-4aed-b625-6877db0d8552",
   "metadata": {},
   "source": [
    "Consider a special trait of podcast transcripts --- the most important information is usually put at the front of an episode. I decided to extract the first K words from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c5564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataframe I already cleaned in last two notebooks\n",
    "train = joblib.load('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac4a5cfe-6950-4e69-b5cf-409c557f545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello. hello. hello everyone. this is katie an...</td>\n",
       "      <td>on the first ever episode of kream in your kof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to inside the 18. today's episode is t...</td>\n",
       "      <td>today’s episode is a sit down michael and omar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey cheese fans before we get started. i wante...</td>\n",
       "      <td>join us as we take a look at all current chief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get ready to whiten those knuckles and hold fa...</td>\n",
       "      <td>former boatswain’s mate dan shirey talks pitch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey everyone, welcome to another episode of th...</td>\n",
       "      <td>how are relationships made? what is trust buil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  hello. hello. hello everyone. this is katie an...   \n",
       "1  welcome to inside the 18. today's episode is t...   \n",
       "2  hey cheese fans before we get started. i wante...   \n",
       "3  get ready to whiten those knuckles and hold fa...   \n",
       "4  hey everyone, welcome to another episode of th...   \n",
       "\n",
       "                                             summary  \n",
       "0  on the first ever episode of kream in your kof...  \n",
       "1  today’s episode is a sit down michael and omar...  \n",
       "2  join us as we take a look at all current chief...  \n",
       "3  former boatswain’s mate dan shirey talks pitch...  \n",
       "4  how are relationships made? what is trust buil...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d55096a-3d5a-4fb6-80b1-42450fa46fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32354, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb87139-938a-4b21-b8a6-257283fc133b",
   "metadata": {},
   "source": [
    "Everything looks right. Now let's set K=100 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80d0b266-074a-49ca-9805-05c3ca37a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=100\n",
    "Baseline_100words = train['transcript'].apply(lambda x : ' '.join(x.split()[:K]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c0ed00-59a8-4f62-87d0-34fd834bf689",
   "metadata": {},
   "source": [
    "Now I can introduce the evaluation metric for automatic summarization --- Rouge score.\n",
    "\n",
    "Rouge-l measures the longest subsequence between the summary and the extractive summary. \n",
    "\n",
    "Rouge-2 measures the number of 2-gram words shared by both summary and extractive summary. \n",
    "\n",
    "Rouge-1 measures the number of single words that appear in both the summary and extractive summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "bac8e76d-dd64-4320-b8d6-922dbba6d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge function returns precision, recall and f1 scores for rouge-1, rouge-2, and rouge-l\n",
    "#\n",
    "def evaluate_summary(y_test, predicted):    \n",
    "    rouge_score = Rouge()    \n",
    "    #get all the rouge score\n",
    "    scores = rouge_score.get_scores(y_test, predicted, avg=True)       \n",
    "    \n",
    "    #get the rouge-1 score\n",
    "    score_1_f = round(scores['rouge-1']['f'], 2)\n",
    "    score_1_r = round(scores['rouge-1']['r'], 2)\n",
    "    score_1_p = round(scores['rouge-1']['p'], 2)\n",
    "    #get the rouge-2 score\n",
    "    score_2_f = round(scores['rouge-2']['f'], 2)\n",
    "    score_2_r = round(scores['rouge-2']['r'], 2)\n",
    "    score_2_p = round(scores['rouge-2']['p'], 2)\n",
    "    #get the rouge-3 score\n",
    "    score_L_f = round(scores['rouge-l']['f'], 2) \n",
    "    score_L_r = round(scores['rouge-l']['r'], 2)\n",
    "    score_L_p = round(scores['rouge-l']['p'], 2)\n",
    "    print(\"rouge1 f1:\", score_1_f, \"| rouge2 f1:\", score_2_f, \"| rougeL f1:\",score_L_f)\n",
    "    print(\"rouge1 recall:\", score_1_r, \"| rouge2 recall:\", score_2_r, \"| rougeL recall:\",score_L_r)\n",
    "    print(\"rouge1 precision:\", score_1_p, \"| rouge2 precision:\", score_2_p, \"| rougeL precision:\",score_L_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38fdadfc-b652-4174-afb5-d8f46cf7d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.22 | rouge2 f1: 0.04 | rougeL f1: 0.2\n",
      "rouge1 recall: 0.27 | rouge2 recall: 0.05 | rougeL recall: 0.23\n",
      "rouge1 precision: 0.22 | rouge2 precision: 0.04 | rougeL precision: 0.19\n"
     ]
    }
   ],
   "source": [
    "#Get the rouge score for the baseline_100words\n",
    "#Reminder, this is going to take a while, around 5 minutes!\n",
    "evaluate_summary(Baseline_100words,train['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b8d90-0d3c-4a8c-8747-a06ce7b01223",
   "metadata": {},
   "source": [
    "The first 100 words already covered 27% of summaries' single words (rouge-1 recall), covered 5% of 2-gram words (rouge-2 recall),\n",
    "and 23% longest subsequence (rouge-1 recall).\n",
    "The first 100 words already did an incredible job in capturing the major summarization information.\n",
    "This is also consistent with real life in that podcasts episodes often give some overviews at the beiginning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c292f3-0bfe-45e6-bbe1-fd9b88e63ae0",
   "metadata": {},
   "source": [
    "Now let's set K=200, and compare the rouge score when K = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b96d101-0a1b-4883-a11d-eaafde2740b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=200\n",
    "Baseline_200words = train['transcript'].apply(lambda x : ' '.join(x.split()[:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfd1efbd-5fd4-47dd-97e8-8789046ef6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.23 | rouge2 f1: 0.05 | rougeL f1: 0.2\n",
      "rouge1 recall: 0.37 | rouge2 recall: 0.09 | rougeL recall: 0.33\n",
      "rouge1 precision: 0.18 | rouge2 precision: 0.04 | rougeL precision: 0.16\n"
     ]
    }
   ],
   "source": [
    "#Get the rouge score for the baseline_200words\n",
    "#Reminder, this is going to take a while, around 10 minutes!\n",
    "evaluate_summary(Baseline_200words,train['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78036e6-085b-41ba-ac6a-d602aefcd91c",
   "metadata": {},
   "source": [
    "Compared to baseline_100words, for baseline_200words, the precision of rouge-1, rouge-2, rouge-l have decreased.\n",
    "The first 200 words cover 37% single words in summaries, 9% 2-gram words in summaries. 33% longest subsequence in summaries.\n",
    "The recall increases because baseline_200words have 2 times the words of baseline_100words.\n",
    "This shows longer summaries tend to capture more phrases and single words.\n",
    "\n",
    "But also, the longer summary gets lower precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b0336-d7c9-4735-896a-83a421f86b3c",
   "metadata": {},
   "source": [
    "Also, when k=100, it is closer aligned with the distribution of the provided summaries.\n",
    "In the transcripts loading notebook, I know the summaries' word counts are between 82 ± 66 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1115e8-2996-4855-b342-2b07f0055c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d29c69cc-31ff-4564-a157-65dc997e24d3",
   "metadata": {},
   "source": [
    "## Build extractive summary model based on TF-IDF\n",
    "\n",
    "In the model, for each transcript, I treat each text as a document, with one transcript as a corpus.\n",
    "I get TFIDF vectorization on each transcript (one corpus),and use token tfidf to calculate the score for each document.\n",
    "Then, sort the document scores, and get the top 3 documents (3 sentences) as the final summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c084bbe4-4d39-4446-b665-4eb57a301908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries we need\n",
    "from nltk.corpus import stopwords   \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d4846-faf3-4ed1-b7f9-8e9761d1113d",
   "metadata": {},
   "source": [
    "First I need to define my tokenizer.\n",
    "\n",
    "I removed stop words, I removed punctuations.\n",
    "I used stemming, which is a rule-based way of cutting off 's', 'ing', and other endings to reduce words to a basic root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2e6d8ecf-04af-4329-b328-7c0b8e4a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in stopwords) and (word!=''):\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "    return listofstemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "05a03029-5624-43ab-94f8-932417e05106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_score_summary(text):\n",
    "    '''\n",
    "    This is a function that did the following three steps:\n",
    "    1, Divide each sentence into a entry or a document\n",
    "    2, Get tfidf matrix for all documents\n",
    "    3, get the score for each documents based on tokens tfidf \n",
    "    4, sort the score descendingly, use the 3 sentences with highest score.\n",
    "    5, return the summary\n",
    "    '''\n",
    "    \n",
    "    sentences=[]\n",
    "    for sent in sent_tokenize(text):\n",
    "        sentences.append(sent)\n",
    "    # get into dataframe, every sentence in the transcript is an entry\n",
    "    df = pd.DataFrame({'sentence':sentences})\n",
    "                      \n",
    "    #initiate a TFIDF vector\n",
    "    vec=TfidfVectorizer(min_df=min_df,tokenizer=my_tokenizer)\n",
    "                      \n",
    "    #fit and transform sentences\n",
    "    sentence_transformed = vec.fit_transform(df['sentence'])\n",
    "                      \n",
    "    #get the TF-IDf matrix for sentences\n",
    "    sentence_df=pd.DataFrame(columns=vec.get_feature_names(), data=sentence_transformed.toarray())\n",
    "    \n",
    "    #calculate the score for each sentence\n",
    "    #sum up all the tf idf score for tokens in a sentence\n",
    "    sentence_df_sum = np.sum(sentence_df,axis=1)\n",
    "    \n",
    "    #the count of tokens that appears in a sentence\n",
    "    #put 0 in where sentence_df != 0\n",
    "    transit = sentence_df.where(sentence_df==0,1)\n",
    "                      \n",
    "    #tokens number in one sentence\n",
    "    sentence_df_num = np.sum(sentence_df,axis=1)          \n",
    "    \n",
    "    #calculate the score for each sentence\n",
    "    sentence_df_score = []\n",
    "    for sum_tokens_score, num_tokens in zip(sentence_df_sum.tolist(), sentence_df_num.tolist()):\n",
    "        if num_tokens != 0:\n",
    "            sentence_df_score.append(round(sum_tokens_score/num_tokens,3))\n",
    "        else:\n",
    "            sentence_df_score.append(0)\n",
    "    #concatenate back the sentence_score to sentence\n",
    "    df['score'] = sentence_df_score\n",
    "    df=df.sort_values('score',ascending=False)\n",
    "    summary = ' '.join(df.head(number_of_sentence)['sentence'])\n",
    "    \n",
    "    return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90650ce-a471-4327-a64f-2d5084bea9ab",
   "metadata": {},
   "source": [
    "When I build the tfidf summrization function, I have trouble in choosing the number of sentences to form a summary.\n",
    "\n",
    "I recall, in the transcripts loading notebook, I found out, \n",
    "for central summaries, the number of sentences are between\n",
    "5 ± 4.\n",
    "\n",
    "Since I already know longer summary always give better recall score, I decide to try 5 and 9, to see if there is a significant difference.\n",
    "\n",
    "Also, I found out when I change min_df to the range from 0 to 5, different summaries get returned. So I decide to use min_df=0, min_df=3, and min_df=5 on a small set of transcripts to see which one performs better.\n",
    "\n",
    "For an extractive summarization model is trained only on one document, there is no information leakage.\n",
    "\n",
    "I can just extract 100 random documents from transcripts, and see how number_of_sentence and min_df values perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "f2b6e3d3-3086-41c9-a3bf-9212a31e2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random_choice=np.random.choice(train.shape[0],100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c797f3-41d9-4c39-bf91-f68361309f7c",
   "metadata": {},
   "source": [
    "\n",
    "Let's do for min_df = 0 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "400e0632-2ea3-437e-9acc-6bee51aa56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=0\n",
    "number_of_sentence = 5 \n",
    "tfidf_0=train['transcript'][random_choice].apply(tfidf_score_summary) #apply on transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d493cf88-cb46-4928-82de-5c6c5096bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.19 | rouge2 f1: 0.02 | rougeL f1: 0.16\n",
      "rouge1 recall: 0.21 | rouge2 recall: 0.02 | rougeL recall: 0.18\n",
      "rouge1 precision: 0.2 | rouge2 precision: 0.02 | rougeL precision: 0.17\n"
     ]
    }
   ],
   "source": [
    "#the zero th experiment\n",
    "evaluate_summary_f1(tfidf_0,train['summary'][random_choice]) #apply on summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "97663830-1f26-408b-bb17-d273aa364f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=0\n",
    "number_of_sentence = 9 \n",
    "tfidf_1=train['transcript'][random_choice].apply(tfidf_score_summary) #apply on transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "afb14cd4-3f75-4255-9b47-403937e07258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.19 | rouge2 f1: 0.02 | rougeL f1: 0.17\n",
      "rouge1 recall: 0.28 | rouge2 recall: 0.03 | rougeL recall: 0.24\n",
      "rouge1 precision: 0.16 | rouge2 precision: 0.01 | rougeL precision: 0.14\n"
     ]
    }
   ],
   "source": [
    "#the first experiment\n",
    "evaluate_summary(tfidf_1,train['summary'][random_choice])  #apply on summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6ef9f-86a0-43fa-91ed-1cecf87d376c",
   "metadata": {},
   "source": [
    "Both shows strong precision.\n",
    "\n",
    "Especially for number_of_sequence as 9, without losing precision, the F1 and recall scores both increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2d69f8ea-d4ac-4961-9fd4-7f2b380909cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean      93.490000\n",
       "std       46.240149\n",
       "min       27.000000\n",
       "25%       56.500000\n",
       "50%       87.000000\n",
       "75%      120.250000\n",
       "max      222.000000\n",
       "Name: transcript, dtype: float64"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_0.apply(lambda x:len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "72894710-83c5-4bf6-be4f-6742e2e20da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean     171.910000\n",
       "std       73.163315\n",
       "min       58.000000\n",
       "25%      123.000000\n",
       "50%      157.000000\n",
       "75%      212.250000\n",
       "max      468.000000\n",
       "Name: transcript, dtype: float64"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1.apply(lambda x:len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6261fa-8d1f-4078-b624-96c72f32a2ab",
   "metadata": {},
   "source": [
    "However, after I checked the word count for both experiments, I realize I can't set the number of sentences as 9.\n",
    "Because target summaries have 82±66 in the central range, the returned summary of number of sentences as 9 has a word range of 123±73.\n",
    "\n",
    "While the number of sentences as 5 have a word range of 87 ± 46, which is much closer to the target summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "4dd8e7f5-3632-475c-941f-360fed528098",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=0\n",
    "number_of_sentence = 6 \n",
    "tfidf_0_1=train['transcript'][random_choice].apply(tfidf_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "08b2eb81-5dee-4860-8a83-5cc47924171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean     113.620000\n",
       "std       54.498963\n",
       "min       35.000000\n",
       "25%       65.750000\n",
       "50%      105.000000\n",
       "75%      145.000000\n",
       "max      279.000000\n",
       "Name: transcript, dtype: float64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_0_1.apply(lambda x:len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f0d78-fafe-4e98-b490-e40a9664eb43",
   "metadata": {},
   "source": [
    "I tried setting the number of sentences as 6: 50% of returned summaries are longer than 105 words. It seems this is still too big to be compatible with target summaries.\n",
    "\n",
    "5 seems to be the best number of sentences value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d7ba25d0-a92f-4ea8-abe5-fc182a4ab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the second experiement\n",
    "min_df=3\n",
    "number_of_sentence = 5 \n",
    "tfidf_2=train['transcript'][random_choice].apply(tfidf_score_summary) #used on transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ad0bb392-aca1-4a15-abb3-a83cfcbd9ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.18 | rouge2 f1: 0.02 | rougeL f1: 0.15\n",
      "rouge1 recall: 0.21 | rouge2 recall: 0.02 | rougeL recall: 0.18\n",
      "rouge1 precision: 0.19 | rouge2 precision: 0.02 | rougeL precision: 0.16\n"
     ]
    }
   ],
   "source": [
    "evaluate_summary(tfidf_2,train['summary'][random_choice]) #apply summary as True summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "0c300930-fc8f-420c-aa7e-e152fbd98403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third experiement\n",
    "min_df=5\n",
    "number_of_sentence = 5 \n",
    "tfidf_3=train['transcript'][random_choice].apply(tfidf_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "865cfaa4-f1f3-45ab-854f-7644e725ee9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.19 | rouge2 f1: 0.02 | rougeL f1: 0.16\n",
      "rouge1 recall: 0.22 | rouge2 recall: 0.02 | rougeL recall: 0.19\n",
      "rouge1 precision: 0.19 | rouge2 precision: 0.02 | rougeL precision: 0.17\n"
     ]
    }
   ],
   "source": [
    "evaluate_summary(tfidf_3,train['summary'][random_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a302b6-2a45-468f-b31b-56358c62ad62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c0bf1-fc4b-47a5-81c4-21a48c4c753f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "401eaf00-85fc-4850-bf4c-09f8b670982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third experiement\n",
    "min_df=0.1\n",
    "number_of_sentence = 5 \n",
    "tfidf_3=train['transcript'][random_choice].apply(tfidf_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "11b543fb-8ae8-40f1-ba6c-3df10af6eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.19 | rouge2 f1: 0.02 | rougeL f1: 0.16\n",
      "rouge1 recall: 0.25 | rouge2 recall: 0.03 | rougeL recall: 0.21\n",
      "rouge1 precision: 0.17 | rouge2 precision: 0.02 | rougeL precision: 0.15\n"
     ]
    }
   ],
   "source": [
    "evaluate_summary(tfidf_3,train['summary'][random_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d9b3b-6df4-4bb2-8ce5-cde507e4398a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4717624-db49-46ba-b20f-7c8576a8a722",
   "metadata": {},
   "source": [
    "I did three combinations in the last few cells.\n",
    "After I settled with number_of_sentence = 5,\n",
    "I applied three min_df values:\n",
    "min_df=0, 3, 5\n",
    "\n",
    "As long as min_df is set higher, Rouge precision will get higher.\n",
    "With the same sentence number of 5, precision gets higher. This means the predicted summary gets closer to the True summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38557f2-5f9b-4f41-90d7-5cab5c2fd84c",
   "metadata": {},
   "source": [
    "\n",
    "I decided to set \n",
    "\n",
    "min_df = 5\n",
    "\n",
    "number_of_sentence=5\n",
    "\n",
    "on the whole dataset, and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e1138-4b92-4b0d-965d-a09d0cb11534",
   "metadata": {},
   "source": [
    "During the final training:\n",
    "\n",
    "I run into error for min_df=[2,3,4,5]. error said after pruning, there are no terms left. I need lower the min_df.\n",
    "Since it can't handle hard numbers, I decide to change it to be a soft proportion of documents.\n",
    "I decide to set it as min_Df=0.1. Of course, this brings an error alert.\n",
    "\n",
    "But when I changed it to 0.05, it works.\n",
    "\n",
    "Maybe this is a limitation of using TFIDF vectorizer as an extractive summary model.\n",
    "\n",
    "Vectorizer will raise errors when one document is empty after pruning.\n",
    "\n",
    "If one sentence full of rare words that shows up less than min_df times in other sentences, from intuition, this sentence shouldn't be chosen as the summary sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "49ecb277-29f1-49ec-b9d0-c051ddb1952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=0\n",
    "number_of_sentence = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "bf79d0a0-fc20-42f4-a5ae-ef2fd3a53d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reminder this cell runs more than 30 minutes.\n",
    "tfidf_summary = train['transcript'].apply(tfidf_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "23df3ca8-46d8-44d6-8e69-7e14690811f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_summary.pkl']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfidf_summary,'tfidf_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "75b66309-80e9-4fe9-96fd-5301114b5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.18 | rouge2 f1: 0.02 | rougeL f1: 0.15\n",
      "rouge1 recall: 0.21 | rouge2 recall: 0.02 | rougeL recall: 0.18\n",
      "rouge1 precision: 0.19 | rouge2 precision: 0.02 | rougeL precision: 0.16\n",
      "\n",
      " duration:148.15238499641418\n"
     ]
    }
   ],
   "source": [
    "#evaluate on the whole dataset.\n",
    "start= time.time()\n",
    "evaluate_summary(tfidf_summary,train['summary'])\n",
    "end=time.time()\n",
    "duration=end-start\n",
    "print(f'\\n duration:{duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d44f4c-49f9-425b-9606-4491407fec87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120bd71f-dc68-4102-8365-669de3287598",
   "metadata": {},
   "source": [
    "Set min_df as soft number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a33dd3bb-ac28-46fc-bd1d-8a7a73cb2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=0.05\n",
    "number_of_sentence=5\n",
    "tfidf_2 = train['transcript'].apply(tfidf_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "4e71c5e8-6da3-4bd5-b9a9-77bc0e96629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_summary_2.pkl']"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tfidf_2,'tfidf_summary_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2ce71232-e69d-40ee-a4a1-cefab98e5327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.18 | rouge2 f1: 0.02 | rougeL f1: 0.16\n",
      "rouge1 recall: 0.23 | rouge2 recall: 0.03 | rougeL recall: 0.19\n",
      "rouge1 precision: 0.18 | rouge2 precision: 0.02 | rougeL precision: 0.15\n",
      "\n",
      " duration:181.35782885551453\n"
     ]
    }
   ],
   "source": [
    "#evaluate on the whole dataset.\n",
    "start= time.time()\n",
    "evaluate_summary(tfidf_2,train['summary'])\n",
    "end=time.time()\n",
    "duration=end-start\n",
    "print(f'\\n duration:{duration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f672e-1fa5-4cc0-ae3b-cc55314b4efc",
   "metadata": {},
   "source": [
    "Compared to min_df=0, when I set min_df=0.05, my rouge recall increases.\n",
    "Rouge precision decreases a little bit.\n",
    "However, the f1 score slightly increases.\n",
    "Overall, I choose 0.05 over 0.\n",
    "\n",
    "\n",
    "Compared to baseline_100words, tfidf_summary seems to perform worse in both recall and precision. Let's compare the real summary manually.\n",
    "\n",
    "First I need to pick a random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "d5edcad7-87fa-45bd-a830-37e4dae18697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random_number = np.random.choice(train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "00495d22-f482-4aea-9f53-671df17de64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thoughts on who we are. what drives us and how we form our identity. i apologize for the massive amount of cuts and poor sound quality but the desire to post the episode and it’s message outweighed the desire to re-record. i was speaking from the heart and it was unscripted.'"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the True summary that is created by the show creator\n",
    "train['summary'][random_number].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "c9d0fe31-b8af-44f4-bfbf-3438a351cd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wanted to start this out by letting you guys hear a couple of the amazing messages. i have received since i started this that that inspire the hell out of me, and it is just so very appreciated. hey, i am a big fan of yours. and i am glad to see that you have started a podcast and i will be listening very rigorously. so enjoy. make the most of it. thank you. keep doing what you are doing, and i am proud of you. hey, it is marci irene. you gave me a shout out and alive the'"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the baseline_100 model\n",
    "Baseline_100words[random_number].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "98b9cc91-77a6-4fb6-8ab8-1aaf54acacbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wanted to start this out by letting you guys hear a couple of the amazing messages. nobody believes bullshit excuses. it will stay with you for years of using years. all the things you did not accomplish all the things you did not do. and in your mind  it will stick with you.'"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_summary[random_number].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "a61d9f41-c223-4257-a7e3-d2161b628670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wanted to start this out by letting you guys hear a couple of the amazing messages. it is something you need to get out of not fall into most people do not have that problem do not have that issue or concern and it is just not in their nature. all the things you did not accomplish all the things you did not do. sometimes there is more than one number one, but you can only do one thing at a time. some things can fall off the priority list.'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2[random_number].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051181b2-0d93-485a-ad91-f07fa9dce768",
   "metadata": {},
   "source": [
    "Because min_df = 0 or 0.05, sometimes the tfidf_summary puts more focus on rare words. In text summarization, in which this is a disaster, I need to fix this.\n",
    "\n",
    "Besides that, I realize I need to do more cleaning of my transcripts, there is still a lot sponsor of information.\n",
    "\n",
    "So, first step: I need go back to the transcript loading notebook and do more cleaning.\n",
    "\n",
    "The second step: I use count vector to build an extractive summary model, that doesn't care about rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166599d-fd91-495a-8efe-563a490fe9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384594b-4daa-4e65-99b3-4f3c69e328e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0941637-5269-48af-9c81-ca221817f679",
   "metadata": {},
   "source": [
    "## Another two extractive summarizers\n",
    "\n",
    "1. Based on Count Vectorizer\n",
    "\n",
    "2. Based on Text Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11428a4a-9848-49c4-b8fa-4e0b32125f08",
   "metadata": {},
   "source": [
    "### Word Frequency based on extractive summary model\n",
    "\n",
    "In this model, I first get the word tokens from one transcript, then score them with their frequency divided by the maximum frequency.\n",
    "Now all word token scores are between 0 and 1.\n",
    "For each sentence, I use the sum of tokens score that belongs to the sentence as the final sentence score.\n",
    "Then, sort by sentence score, and extract the top n sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "4687ee14-288a-4f9d-b242-a16442be6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def countvec_score_summary(text):\n",
    "    '''\n",
    "    this function did the following steps:\n",
    "    1, treat every sentence as a document, and count vectorizer on whole corpus\n",
    "    2, use the token frequency /max token frequency as token score\n",
    "    3, turn sentence count vectorizer matrix to binary matrix, use dot prodcut to calculate the score for each sentence\n",
    "    4, rank sentences by score, choose the top n sentences\n",
    "    '''\n",
    "    #step 1 get count vectorizer\n",
    "    sentences=[]\n",
    "    for sent in sent_tokenize(text):\n",
    "        sentences.append(sent)\n",
    "    # get into dataframe, every sentence in the transcript is an entry\n",
    "    df = pd.DataFrame({'sentence':sentences})\n",
    "    vec = CountVectorizer(tokenizer=my_tokenizer,max_features=5000) #set as 5000, to decrease the running time\n",
    "    transformed = vec.fit_transform(df['sentence'])\n",
    "    #put back to dataframe\n",
    "    sentence_df = pd.DataFrame(columns=vec.get_feature_names(), data=transformed.toarray())\n",
    "\n",
    "    sentence_df_sum = np.sum(sentence_df,axis=0)\n",
    "\n",
    "    max_frequency = np.max(sentence_df_sum)\n",
    "    #get score for each token\n",
    "    sentence_df_sum=sentence_df_sum/max_frequency\n",
    "    # put binary value back \n",
    "    #for one sentence, if the token in it, the value is one instead of frequency\n",
    "    sentence_df=sentence_df.where(sentence_df==0,1)\n",
    "\n",
    "    # #use dot product to get sentence score \n",
    "    sentence_df_sum = sentence_df_sum.to_numpy().reshape(-1,1)\n",
    "    sentence_df = sentence_df.to_numpy()\n",
    "    sentence_score = np.matmul(sentence_df, sentence_df_sum)\n",
    "\n",
    "    # concatenate score and sentence\n",
    "    number_of_sentences=5\n",
    "    df['score']= sentence_score\n",
    "    df=df.sort_values('score',ascending=False)\n",
    "    summary = ' '.join(df.head(number_of_sentences)['sentence'])\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "d06595df-6863-482b-acb7-ba232b152bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random_choice=np.random.choice(train.shape[0],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "e24cf25f-197d-4487-903a-5c4311a89270",
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec_1=train['transcript'][random_choice].apply(countvec_score_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "e80fdb82-ef1a-4042-887a-592841de121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.17 | rouge2 f1: 0.02 | rougeL f1: 0.14\n",
      "rouge1 recall: 0.38 | rouge2 recall: 0.06 | rougeL recall: 0.31\n",
      "rouge1 precision: 0.12 | rouge2 precision: 0.01 | rougeL precision: 0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate_summary(countvec_1,train['summary'][random_choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34fbee-bfd2-4f88-8224-66990178b608",
   "metadata": {},
   "source": [
    "Count vec based extractive model shows great score on recall, but shows the same for f1 because of the low precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d173b6f-2a0e-49b3-9ad6-04c2972f331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689d6e7-d814-4c74-86d5-7f82058e3d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39920ab-7131-473d-b681-7040a39d271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa142e-20a1-4e1c-9735-94df3e4016f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3c2c491-3375-48a9-a257-56d339bf555d",
   "metadata": {},
   "source": [
    "### Text rank model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e487d-e231-4df8-8f16-9955fd22466f",
   "metadata": {},
   "source": [
    "Texxt rank is a graph based ranking model for text processing.\n",
    "\n",
    "Graph based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. \n",
    "\n",
    "When one vertex links to anther one, it is bascically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "e6b5f4f3-1400-46ea-a484-653f16ea31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79de1e-8a28-4168-82dd-66e7b8a6d870",
   "metadata": {},
   "source": [
    "Based on EDA I did on sentence counts in transcripts,\n",
    "I know for cental transcripts, the number of sentences are between 408 ± 264\n",
    "and for central summaries the number of sentences are between 5 ± 4.\n",
    "I set ratio=0.01, which would be compatible with the True summaries.\n",
    "\n",
    "\n",
    "I also know the central summaries have 82 ± 66 words. So I set word_count = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "d29cf92c-7bdf-457e-9532-c38c83c4a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count determines how many the output will contain \n",
    "#percentage of the number of sentences of the original text to be chosen for the summary\n",
    "#when both are provided, the ratio will be ignored\n",
    "\n",
    "#\n",
    "def textrank(text, word_count=100, ratio = 0.01): \n",
    "    summary = gensim.summarization.summarize(text,word_count=word_count)\n",
    "    if len(summary.split())==0:\n",
    "        summary = gensim.summarization.summarize(text,ratio=ratio)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "c7d9255f-fa11-4f3a-acd7-073e3d7632e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random_choice=np.random.choice(train.shape[0],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "f60d3edb-9824-42b4-a75c-fa2dc6af2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_1=train['transcript'][random_choice].apply(textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "0f9a8484-6c01-4ec0-a3df-8061a7e7af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1 f1: 0.22 | rouge2 f1: 0.03 | rougeL f1: 0.17\n",
      "rouge1 recall: 0.26 | rouge2 recall: 0.04 | rougeL recall: 0.2\n",
      "rouge1 precision: 0.21 | rouge2 precision: 0.03 | rougeL precision: 0.16\n"
     ]
    }
   ],
   "source": [
    "evaluate_summary(textrank_1,train['summary'][random_choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1c28d-2831-42ae-b885-4e90c8befc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf727134-94d9-463c-8785-b2316f99c1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a278741-370e-493d-8099-0d78b9e3dd07",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef38ef-d04a-42cf-b813-70b445fb861b",
   "metadata": {},
   "source": [
    "In this notebook, I performed baseline model building and built three different extractive models.\n",
    "\n",
    "The baseline_100words performs the best.\n",
    "Then comes the text rank model, followed by the count vectorizer based model, followed by the tfidf vectorizer based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d4794-4813-444e-9761-2554b7cd8229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6224495a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "432f7402-bc84-4856-b399-bf75790e111b",
   "metadata": {},
   "source": [
    "Reference\n",
    "\n",
    "[1][TextRank: Bringing Order into Text](https://aclanthology.org/W04-3252) (Mihalcea & Tarau, 2004)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f2dfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965da31-ccc0-4b89-ae56-2605db2290e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
